FROM nvcr.io/nvidia/pytorch:25.12-py3 AS builder

# This assumes at least equivalent hardware to what's in the Substack guide: https://www.yevelations.com/p/dev-from-scratch-1n
WORKDIR /build

RUN apt-get update && apt-get install -y \
    git cmake build-essential ninja-build \
    && rm -rf /var/lib/apt/lists/*

RUN pip install --no-cache-dir \
    setuptools-git-versioning \
    scikit-build \
    jinja2 \
    ninja

ARG FBGEMM_COMMIT="de7ef191ee19423a93c453c39915dd6176f56919"
ARG TORCH_CUDA_ARCH_LIST="12.0"
ARG CUDA_ARCH_LIST="12.0"
ENV CUDA_HOME="/usr/local/cuda"

RUN git clone --recursive https://github.com/pytorch/FBGEMM.git && \
    cd FBGEMM && \
    git checkout ${FBGEMM_COMMIT} && \
    git submodule sync && \
    git submodule update --init --recursive

ARG MAX_JOBS="28"
ARG NVCC_THREADS="28"

RUN cd FBGEMM/fbgemm_gpu && \
    sed -i 's/default="none"/default="genai"/g' setup.py && \
    sed -i "s/default='none'/default='genai'/g" setup.py && \
    python setup.py bdist_wheel \
      --nvml_lib_path=/usr/local/cuda/lib64/stubs/libnvidia-ml.so && \
    cp dist/*.whl /build/

FROM nvcr.io/nvidia/pytorch:25.12-py3

WORKDIR /app

COPY --from=builder /build/*.whl /tmp/
RUN pip install /tmp/*.whl && rm /tmp/*.whl

RUN pip install --no-cache-dir \
    fastapi uvicorn httpx pydantic \
    transformers accelerate

COPY docker/manager.py /app/manager.py

CMD ["uvicorn", "manager:app", "--host", "0.0.0.0", "--port", "8000"]